{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-climb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controlling-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imutils\n",
    "from pyModbusTCP.client import ModbusClient\n",
    "from pyModbusTCP import utils\n",
    "import keyboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-angel",
   "metadata": {},
   "source": [
    "### Important points regarding Rotation Matrix\n",
    "* cos(-a) = cos(a)\n",
    "* sin(-a) = sin(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "parallel-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robot_to_camera(robot,a):\n",
    "    \"\"\"Converting Robot coordinate system to Camera (Image plane) coordinate system\n",
    "    -- The translation matrix with 3x1 size has all coordinates in millimeters not meters\n",
    "    \n",
    "    Explanation:\n",
    "            We are giving object coordinates w.r.t robot and converting them to camera coordinates, so we need translation \n",
    "            matrix(4x1) is the how far the robot from camera in camera coordinate system... think in this way: we want to \n",
    "            transform the robot coordinates in camera, that needs how robot origin w.r.t camera origin.\n",
    "    \"\"\"\n",
    "    # converting angle(in radians) to degrees\n",
    "    a = (a / 180) * np.pi\n",
    "\n",
    "    # For this usecase the camera coordiante system is 180 degree rotated in x axis, that's why we only perform Rotation w.r.t X\n",
    "    R_x = np.array([[1, 0, 0], [0, np.cos(a), -np.sin(a)], [0, np.sin(a), np.cos(a)], [0,0,0]])\n",
    "    # robot coordinates in camera(image plan) coordinate system, Camera(Image plane(in mm)) center is taken as the reference for these measured values\n",
    "    T_c = np.array([[-500],[ -21], [540.2], [1]])   \n",
    "    \n",
    "    # calculating Translation matrix \n",
    "    R_Translation_C = np.concatenate((R_x, T_c), axis = 1)\n",
    "    \n",
    "    # finally multiplying the robot coordinates to Transformation matrix\n",
    "    return np.matmul(R_Translation_C, robot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-grant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "forbidden-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_to_robot(camera,a):\n",
    "    \"\"\"Converting Camera (Image plane) coordinate system to Robot coordinate system\n",
    "    -- The translation matrix with 3x1 size has all coordinates in millimeters not meters\n",
    "    \n",
    "    Explanation:\n",
    "            We are giving object coordinates w.r.t robot and converting them to camera coordinates, so we need translation \n",
    "            matrix(4x1) is the how far the robot from camera in camera coordinate system... think in this way: we want to \n",
    "            transform the robot coordinates in camera, that needs how robot origin w.r.t camera origin.\n",
    "    \"\"\"\n",
    "    # setting size of camera\n",
    "    camera = np.transpose(camera)\n",
    "    if camera.shape[0] == 3:\n",
    "        camera = np.concatenate((camera, np.array([1.0])), axis = 0)   # shape = (4,) 1D array\n",
    "        \n",
    "    # converting angle(in radians) to degrees\n",
    "    a = (a / 180) * np.pi\n",
    "\n",
    "    # For this usecase the camera coordiante system is 180 degree rotated in x axis, that's why we only perform Rotation w.r.t X\n",
    "    R_x = np.array([[1, 0, 0], [0, np.cos(a), -np.sin(a)], [0, np.sin(a), np.cos(a)], [0,0,0]])\n",
    "    # robot coordinates in camera(image plan) coordinate system, Camera(Image plane(in mm)) center is taken as the reference for these measured values\n",
    "    #T_c = np.array([[-508],[ -17], [808.2], [1]])    # np.array([[-494.5],[ -19.5], [540.2], [1]])\n",
    "    T_c = np.array([[-530],[ 5.5], [856.2], [1]]) \n",
    "    \n",
    "    # calculating Translation matrix \n",
    "    R_Translation_C = np.concatenate((R_x, T_c), axis = 1)\n",
    "    \n",
    "    # To get the object coordinates in robot, we need to use inverse of the robot to camera transformation matrix\n",
    "    C_Translation_R = np.linalg.inv(R_Translation_C)\n",
    "    \n",
    "    # finally multiplying the robot coordinates to Transformation matrix\n",
    "    return np.matmul(C_Translation_R, camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-wonder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-knight",
   "metadata": {},
   "source": [
    "### Camera Intrinsics and Extrinsics, Finding 3D coordinates from pixel coordinates and Vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "minimal-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_coord(three_D_coord, color_frame):\n",
    "    \n",
    "    \"\"\" it takes 3D world coordinates and returns 2D pixel coordinates on sensor plane\n",
    "    [x_3d, y_3d, z_3d] = three_D_coord\n",
    "    \n",
    "    x_3d = -0.22   - \n",
    "    y_3d = \n",
    "    z_3d = 0.6602\n",
    "    \n",
    "    all dimenstions are in meters, because the algo inside pyrealsense takes measurments in meters,\n",
    "    if we give them in mm, then we will map our inputs in out of FOV plane\"\"\"\n",
    "    \n",
    "    profile = color_frame.get_profile().as_video_stream_profile() \n",
    "#     x,y = rs.rs2_project_point_to_pixel(profile.get_intrinsics(),[0.345,0.242,0.6602])\n",
    "#     x,y = rs.rs2_project_point_to_pixel(profile.get_intrinsics(),[0.1053,-0.0495,0.6602])\n",
    "    pixel = rs.rs2_project_point_to_pixel(profile.get_intrinsics(),three_D_coord)\n",
    "    \n",
    "    return pixel\n",
    "\n",
    "\n",
    "def three_D_coord(pixel, Depth, color_frame):\n",
    "    \n",
    "    \"\"\" it takes pixel coordinates and returns 3D coordinates in the relation of Depth information\n",
    "    [x_p, y_p] = pixel\n",
    "    Depth = 0.6602 with current set up\n",
    "    x_p = between 0-640\n",
    "    y_p = between 0-480  \n",
    "    \n",
    "    all dimenstions are in meters, because the algo inside pyrealsense takes measurments in meters,\n",
    "    if we give them in mm, then we will map our inputs in out of FOV plane\"\"\"\n",
    "    profile = color_frame.get_profile().as_video_stream_profile() \n",
    "\n",
    "#     D_coordinates = rs.rs2_deproject_pixel_to_point(profile.get_intrinsics(),[0,480], 0.6602)\n",
    "    D_coordinates = rs.rs2_deproject_pixel_to_point(profile.get_intrinsics(),pixel, Depth)\n",
    "    D_coordinates = np.array(D_coordinates)\n",
    "    \n",
    "    # To transfer coordinates to PLC, we need to transfer them from meters to millimeters\n",
    "    D_coordinates = D_coordinates*1000\n",
    "    \n",
    "    return D_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-municipality",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "delayed-extent",
   "metadata": {},
   "source": [
    "### Method for saving all 3D contour coordinates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baking-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_to_3D(contours, color_frame):\n",
    "    world = []\n",
    "    profile = color_frame.get_profile().as_video_stream_profile() \n",
    "    for each in contours:\n",
    "        D_coordinates = three_D_coord(each,0.6962, color_frame)\n",
    "        D_coordinates = camera_to_robot(D_coordinates,180)\n",
    "        world.append(D_coordinates[0:3])\n",
    "\n",
    "    return world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-kingdom",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "miniature-suite",
   "metadata": {},
   "source": [
    "### ModbusConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "increased-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modbus():\n",
    "    client_slave = ModbusClient(host = \"194.94.86.6\", port = 502, auto_open = True)\n",
    "    client_slave.open()\n",
    "    if client_slave.is_open():\n",
    "        print(\"connected\")\n",
    "    else:\n",
    "        print(\"not connected, try again\")\n",
    "    return client_slave\n",
    "        \n",
    "def write_registers(list, Client):\n",
    "    for each,s_address in zip(list,range(len(list))):\n",
    "        for value,address in zip(each, range(len(each))):\n",
    "            holding_register_value = int(value*10)\n",
    "            address = address+(s_address*3)\n",
    "            Client.write_single_register(24576+address, utils.get_2comp(holding_register_value))\n",
    "                       \n",
    "#     print(Client.read_holding_registers(0,17), \"Hodling_Registers\")  # in modbus library the address must be an variable, indirect variables are not taken\n",
    "        \n",
    "def close_modbus(Client):\n",
    "    Client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-bradley",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "threatened-norwegian",
   "metadata": {},
   "source": [
    "### Camera Streaming and Contour Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "mental-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Camera_Module():\n",
    "    \n",
    "    # Configure depth and color streams\n",
    "    pipeline = rs.pipeline()\n",
    "    config = rs.config()\n",
    "\n",
    "    # Get device product line for setting a supporting resolution\n",
    "    pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
    "    pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "    device = pipeline_profile.get_device()\n",
    "    device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
    "\n",
    "    found_rgb = False\n",
    "    for s in device.sensors:\n",
    "        if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
    "            found_rgb = True\n",
    "            break\n",
    "    if not found_rgb:\n",
    "        print(\"The demo requires Depth camera with Color sensor\")\n",
    "        exit(0)\n",
    "\n",
    "    config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "\n",
    "    if device_product_line == 'L500':\n",
    "        config.enable_stream(rs.stream.color, 960, 540, rs.format.bgr8, 30)\n",
    "    else:\n",
    "        config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "    # Start streaming\n",
    "    pipeline.start(config)\n",
    "    \n",
    "    # Start Modbus Communication\n",
    "    client = modbus()\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            \n",
    "            # Wait for a coherent pair of frames: depth and color\n",
    "            frames = pipeline.wait_for_frames()\n",
    "            color_frame = frames.get_color_frame()\n",
    "            depth_frame = frames.get_depth_frame()\n",
    "            if not color_frame:\n",
    "                continue\n",
    "\n",
    "            # Convert images to numpy arrays\n",
    "            color_image = np.asanyarray(color_frame.get_data())\n",
    "            depth_image = np.asanyarray(depth_frame.get_data())\n",
    "            color_image = cv2.flip(color_image,-1)\n",
    "            \n",
    "           \n",
    "            \n",
    "            # Modbus Homed Command\n",
    "            homed = client.read_discrete_inputs(0,1)\n",
    "            #print(homed[0])\n",
    "            \n",
    "            \n",
    "            # start of Contour Detection\n",
    "            conbri_image = cv2.convertScaleAbs(color_image, alpha=2.1, beta=35)\n",
    "            grayimage = cv2.cvtColor(conbri_image, cv2.COLOR_BGR2GRAY)\n",
    "            ret, thresh_img =  cv2.threshold(grayimage, 211, 255, cv2.THRESH_BINARY_INV)  #cv2.threshold(grayimage, 89, 255, cv2.THRESH_BINARY) oldone\n",
    "            contours, hierarchy = cv2.findContours(thresh_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "            \n",
    "#             grayimage = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)\n",
    "#             ret, thresh_img = cv2.threshold(grayimage, 65, 200, cv2.THRESH_BINARY)\n",
    "#             contours, hierarchy = cv2.findContours(thresh_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    #         contours = imutils.grab_contours(contours)\n",
    "#             cnts_sorted = sorted(contours, key=lambda x: cv2.contourArea(x))\n",
    "\n",
    "            list_of_contours = []\n",
    "            for c in contours:\n",
    "                area = cv2.contourArea(c)\n",
    "    #             if area < 175 and area > 120 and x < 2:\n",
    "                if area > 500 and area < 50000:\n",
    "#                 if area > 9000:\n",
    "\n",
    "                    M = cv2.moments(c)\n",
    "                    # Finding centroid\n",
    "                    cX = int(M[\"m10\"]/ M[\"m00\"])\n",
    "                    cY = int(M[\"m01\"]/ M[\"m00\"])\n",
    "                    list_of_contours.append([cX, cY])\n",
    "                    cv2.drawContours(color_image, [c], -1,(0,255,0), 1)\n",
    "                    cv2.circle(color_image, (cX, cY), 1, (0,255,255), -1)\n",
    "\n",
    "            # End of Contour Detection\n",
    "\n",
    "            # Finding pixel coordinates from 3D coordinates( in meters)\n",
    "            x,y = pixel_coord([0.1053,-0.0495,0.6962], color_frame)\n",
    "            cv2.circle(color_image, [int(x),int(y)], 1, (0,0,255), -1)\n",
    "            # Drawing center at the principle point \n",
    "            cv2.circle(color_image, [321,232], 1, (255,0,255), -1)\n",
    "#             cv2.circle(color_image, [320,240], 1, (0,255,255), -1)\n",
    "\n",
    "            # Finding 3D world coordinates from pixel coordinates\n",
    "#             D_coordinates = three_D_coord([0,240], 0.6602, color_frame)  # These values in millimeters\n",
    "            _3_D_coord = pixel_to_3D(list_of_contours, color_frame)\n",
    "\n",
    "            #cv2.imshow('Contour Detection', color_image)\n",
    "             # Displaying image\n",
    "            cv2.imshow('RealSense', color_image)\n",
    "            cv2.imshow(\"const\", conbri_image)\n",
    "            cv2.imshow(\"thresh\", thresh_img)\n",
    "            \n",
    "            # Breaking the Streaming    \n",
    "            key = cv2.waitKey(1)\n",
    "            if key == 27:\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "            \n",
    "            if homed[0]:\n",
    "                # Writing back the Modbus Values\n",
    "                write_registers(_3_D_coord, client)\n",
    "            \n",
    "            else:\n",
    "                continue\n",
    "             \n",
    "            #This is for safe side, it can be used later for breaking the while loop\n",
    "#             if keyboard.is_pressed(\"q\"):\n",
    "#                 break\n",
    "\n",
    "        # Destroying all windows after disrupting streaming\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # Closing Modbus Connection\n",
    "        close_modbus(client)\n",
    "        \n",
    "        \n",
    "        # For debugging\n",
    "        print(x,y)\n",
    "        print(len(_3_D_coord)*3, \"total number of holding registers are written\")\n",
    "        print(_3_D_coord)\n",
    "       \n",
    "        \n",
    "\n",
    "    #finally:\n",
    "    except KeyboardInterrupt:\n",
    "        # Stop streaming\n",
    "        pipeline.stop()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "exterior-major",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected\n",
      "412.14501953125 189.3399658203125\n",
      "3 total number of holding registers are written\n",
      "[array([459.03064191,  23.16746305, 159.99998684])]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    Camera_Module()\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-bhutan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-class",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-cleanup",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
